# VectorAccelerate Learning Guide

> **Unleash the GPU‚Äîtaking vector search from fast to massively parallel.**

Welcome to the VectorAccelerate Learning Guide. This is the **third volume** in the VSK (Vector Search Kit) educational series, designed to teach you how to leverage Apple Silicon GPUs for dramatic performance improvements in similarity search.

---

## Prerequisites: VectorCore + VectorIndex Foundations

**This guide assumes you have completed both previous volumes:**
- [VectorCore Learning Guide](../../VectorCore/Guides/00-Welcome.md) (Volume 1)
- [VectorIndex Learning Guide](../../VectorIndex/Guides/00-Welcome.md) (Volume 2)

The concepts build directly on each other:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        VectorCore (Volume 1)                            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   Memory ‚Üí SIMD ‚Üí Numerical ‚Üí Unsafe ‚Üí Performance ‚Üí Capstone           ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   "How do I make ONE distance computation fast?"                        ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       VectorIndex (Volume 2)                            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   Similarity ‚Üí Flat ‚Üí IVF ‚Üí HNSW ‚Üí PQ ‚Üí Tuning ‚Üí Capstone               ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   "How do I search MILLIONS of vectors on CPU?"                         ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     VectorAccelerate (Volume 3)                         ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   GPU Basics ‚Üí Distance Kernels ‚Üí Search ‚Üí Memory ‚Üí Pipelines ‚Üí Capstone‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   "How do I use the GPU to go EVEN FASTER?"                             ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Metal 4 & MSL 4.0

VectorAccelerate is built on **Metal 4**, Apple's latest GPU programming framework. This brings several key improvements:

### Platform Requirements

```
Target Platforms:
  macOS 26.0+       (Tahoe)
  iOS 26.0+
  visionOS 3.0+

MSL Version: 4.0 (Metal Shading Language)
```

### Metal 4 Features Used in VectorAccelerate

| Feature | How It's Used | Benefit |
|---------|--------------|---------|
| **SIMD Group Intrinsics** | `simd_shuffle_xor`, `simd_sum`, `simd_min` | Efficient warp-level communication for TopK selection |
| **Unified Memory** | `.storageModeShared` buffers | Zero-copy CPU‚ÜîGPU on Apple Silicon |
| **ResidencySet** | Explicit memory residency | Predictable performance, controlled eviction |
| **Argument Tables** | Efficient buffer binding | Reduced CPU overhead per dispatch |
| **MSL 4.0 Compiler** | Improved optimization | Better code generation for complex kernels |

### Version Detection in Shaders

```metal
// üìç See: Sources/VectorAccelerate/Metal/Shaders/Metal4Common.h:38-50

#if __METAL_VERSION__ >= 400
    #define VA_MSL_4_0 1
    #define VA_METAL_4_AVAILABLE 1
#else
    #define VA_MSL_4_0 0
    #define VA_METAL_4_AVAILABLE 0
#endif
```

VectorAccelerate requires Metal 4 and has no backwards compatibility with Metal 3 or earlier. Ensure your target platforms meet the requirements above.

---

### From VectorCore, You'll Use:

| VectorCore Concept | How It Applies to GPU |
|-------------------|----------------------|
| SIMD operations | GPU SIMD lanes work similarly‚Äîbut with 32+ lanes instead of 4 |
| Memory layout & alignment | GPU buffers need proper alignment for coalesced access |
| Numerical stability | Parallel reductions require careful accumulation order |
| Unsafe pointers | Metal buffers use raw memory, similar patterns apply |

### From VectorIndex, You'll Use:

| VectorIndex Concept | How It's Accelerated |
|--------------------|---------------------|
| Distance metrics (L2, Cosine) | GPU distance kernels compute millions of pairs in parallel |
| Flat index brute force | GPU handles exhaustive search efficiently for <100K vectors |
| IVF clustering | GPU-accelerated K-Means and centroid assignment |
| Top-K selection | Parallel selection algorithms on GPU |
| Recall vs. latency tradeoffs | GPU adds a new dimension: transfer cost vs. compute benefit |

---

## What You'll Learn

This guide teaches **GPU programming for vector search**‚Äîknowledge that applies to any compute-intensive workload on Apple Silicon:

| Chapter | You'll Learn | Why It Matters |
|---------|-------------|----------------|
| [1. GPU Fundamentals](./01-GPU-Fundamentals/README.md) | Metal compute basics, threadgroups, memory | Understanding the GPU execution model |
| [2. Distance Kernels](./02-Distance-Kernels/README.md) | GPU distance computation, SIMD lanes, reduction | The hot path of similarity search |
| [3. Accelerated Search](./03-Accelerated-Search/README.md) | Top-K on GPU, fused kernels, hybrid strategies | Complete search acceleration |
| [4. Memory Management](./04-Memory-Management/README.md) | Buffers, residency, unified memory | Efficient GPU resource usage |
| [5. Pipeline Optimization](./05-Pipeline-Optimization/README.md) | Kernel fusion, async compute, profiling | Production-ready performance |
| [6. Capstone](./06-Capstone/README.md) | AcceleratedVectorIndex deep dive | Building a complete GPU-first index |

---

## The Core Problem

VectorIndex taught you algorithms that search millions of vectors on CPU. But what if even *those* aren't fast enough?

```
VectorCore (CPU SIMD):
  Single dot product:           ~100 ns

VectorIndex (CPU):
  1M √ó 768D brute force:        ~850 ms
  HNSW search (ef=64):          ~2 ms
  IVF search (nprobe=16):       ~15 ms

VectorAccelerate (GPU):
  1M √ó 768D brute force:        ~12 ms   (70√ó faster!)
  10K candidate reranking:      ~0.3 ms  (7√ó faster!)
```

The GPU isn't magic‚Äîit's **massive parallelism**. Where your CPU has 8-12 cores, Apple Silicon GPUs have thousands of execution units running in lockstep.

---

## When GPU Acceleration Helps

GPU acceleration isn't always the answer. Understanding *when* to use it is crucial:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GPU ACCELERATION DECISION TREE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  Is your workload compute-bound?                                       ‚îÇ
‚îÇ       ‚îÇ                                                                ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ NO ‚Üí CPU is probably fine                                    ‚îÇ
‚îÇ       ‚îÇ                                                                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ YES ‚Üí Continue ‚Üì                                             ‚îÇ
‚îÇ                    ‚îÇ                                                   ‚îÇ
‚îÇ  Is the data already on GPU or easily batched?                         ‚îÇ
‚îÇ       ‚îÇ                                                                ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ NO ‚Üí Transfer costs may dominate                             ‚îÇ
‚îÇ       ‚îÇ         Consider: Can you batch queries?                       ‚îÇ
‚îÇ       ‚îÇ                                                                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ YES ‚Üí Continue ‚Üì                                             ‚îÇ
‚îÇ                    ‚îÇ                                                   ‚îÇ
‚îÇ  Is there enough parallelism?                                          ‚îÇ
‚îÇ       ‚îÇ                                                                ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ < 1K operations ‚Üí GPU overhead too high                      ‚îÇ
‚îÇ       ‚îÇ                                                                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ > 10K operations ‚Üí GPU likely wins                           ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### When to Use GPU

‚úÖ **Batch queries**: Amortize transfer overhead across many queries
‚úÖ **Large candidate sets**: >10K vectors to compare
‚úÖ **High dimensions**: 512D+ embeddings where FLOPS dominate
‚úÖ **Data already on GPU**: Index stored in GPU memory
‚úÖ **Throughput over latency**: Maximize queries/second

### When to Stay on CPU

‚ùå **Single low-latency queries**: GPU launch overhead too high (~50-100Œºs)
‚ùå **Small indices**: <10K vectors, CPU is fast enough
‚ùå **Memory-constrained devices**: GPU memory is limited
‚ùå **Complex branching logic**: GPUs dislike divergent control flow
‚ùå **HNSW graph traversal**: Inherently sequential, hard to parallelize

---

## VectorAccelerate Architecture

VectorAccelerate provides two layers:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    HIGH-LEVEL API                                        ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ              AcceleratedVectorIndex                              ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ GPU-first vector storage                                     ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Flat and IVF index types                                     ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Automatic CPU/GPU routing                                    ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Handle-based vector identification                           ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    LOW-LEVEL API                                        ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ Distance     ‚îÇ ‚îÇ Selection    ‚îÇ ‚îÇ Quantization ‚îÇ ‚îÇ Matrix       ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ Kernels      ‚îÇ ‚îÇ Kernels      ‚îÇ ‚îÇ Kernels      ‚îÇ ‚îÇ Kernels      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ L2         ‚îÇ ‚îÇ ‚Ä¢ TopK       ‚îÇ ‚îÇ ‚Ä¢ Scalar     ‚îÇ ‚îÇ ‚Ä¢ Multiply   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ Cosine     ‚îÇ ‚îÇ ‚Ä¢ FusedL2TopK‚îÇ ‚îÇ ‚Ä¢ Binary     ‚îÇ ‚îÇ ‚Ä¢ Transpose  ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ DotProduct ‚îÇ ‚îÇ ‚Ä¢ WarpSelect ‚îÇ ‚îÇ ‚Ä¢ Product    ‚îÇ ‚îÇ ‚Ä¢ MatVec     ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ Hamming    ‚îÇ ‚îÇ              ‚îÇ ‚îÇ ‚Ä¢ Neural     ‚îÇ ‚îÇ ‚Ä¢ Batch      ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ              Metal4Context                                       ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Device & command queue management                            ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Buffer pools & residency                                     ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Pipeline caching                                             ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Shader compilation                                           ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## How to Use This Guide

### The Sequential Path

Each chapter builds on the previous. If you're new to GPU programming:

```
Chapter 1 ‚îÄ‚îÄ‚Üí Chapter 2 ‚îÄ‚îÄ‚Üí Chapter 3 ‚îÄ‚îÄ‚Üí Chapter 4 ‚îÄ‚îÄ‚Üí Chapter 5 ‚îÄ‚îÄ‚Üí Chapter 6
GPU Basics   Distance     Search       Memory       Pipelines    Capstone
             Kernels      Acceleration Management   & Profiling
```

### The Reference Path

If you're already familiar with Metal or GPU programming:

- **"I want to understand VectorAccelerate's kernels"** ‚Üí [Chapter 2](./02-Distance-Kernels/README.md)
- **"How do I accelerate an existing index?"** ‚Üí [Chapter 3](./03-Accelerated-Search/README.md)
- **"I'm hitting memory issues"** ‚Üí [Chapter 4](./04-Memory-Management/README.md)
- **"I want to see the full implementation"** ‚Üí [Chapter 6](./06-Capstone/README.md)

### Each Guide Follows This Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  THE CONCEPT                                                    ‚îÇ
‚îÇ  What's the GPU technique? Plain English, diagrams.             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  WHY IT MATTERS                                                 ‚îÇ
‚îÇ  What performance problem does this solve?                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  THE TECHNIQUE                                                  ‚îÇ
‚îÇ  Step-by-step with Metal shader code and Swift wrappers.        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  IN VECTORACCELERATE                                            ‚îÇ
‚îÇ  Where is this implemented? Links to actual source.             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üîó VECTORCORE CONNECTION                                       ‚îÇ
‚îÇ  How does this relate to CPU SIMD from Volume 1?                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üîó VECTORINDEX CONNECTION                                      ‚îÇ
‚îÇ  How does this accelerate algorithms from Volume 2?             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  KEY TAKEAWAYS                                                  ‚îÇ
‚îÇ  What should stick? The transferable lessons.                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## VectorAccelerate Source Locations

Throughout this guide, we reference actual implementation code:

| Topic | File Path |
|-------|-----------|
| L2 Distance Kernel | `Sources/VectorAccelerate/Kernels/Metal4/L2DistanceKernel.swift` |
| L2 Distance Shader | `Sources/VectorAccelerate/Metal/Shaders/L2Distance.metal` |
| Cosine Similarity Kernel | `Sources/VectorAccelerate/Kernels/Metal4/CosineSimilarityKernel.swift` |
| Top-K Selection | `Sources/VectorAccelerate/Kernels/Metal4/TopKSelectionKernel.swift` |
| Fused L2+TopK | `Sources/VectorAccelerate/Kernels/Metal4/FusedL2TopKKernel.swift` |
| Streaming TopK | `Sources/VectorAccelerate/Kernels/Metal4/StreamingTopKKernel.swift` |
| Metal4 Context | `Sources/VectorAccelerate/Core/Metal4Context.swift` |
| Residency Manager | `Sources/VectorAccelerate/Core/ResidencyManager.swift` |
| Buffer Pool | `Sources/VectorAccelerate/Core/BufferPool.swift` |
| Accelerated Index | `Sources/VectorAccelerate/Index/AcceleratedVectorIndex.swift` |
| IVF Pipeline | `Sources/VectorAccelerate/Index/Kernels/IVF/IVFSearchPipeline.swift` |

---

## Notation Conventions

| Symbol | Meaning |
|--------|---------|
| `Q` | Number of query vectors |
| `N` | Number of database vectors |
| `D` | Vector dimension |
| `K` | Number of nearest neighbors |
| `üìç See:` | Link to VectorAccelerate source code |
| `üîó VectorCore:` | Connection to VectorCore concept |
| `üîó VectorIndex:` | Connection to VectorIndex concept |
| `‚ö†Ô∏è` | Common mistake or pitfall |
| `üí°` | Key insight or tip |
| `üéØ` | Performance optimization |

---

## Hardware Requirements

VectorAccelerate requires **Metal 4** features available on:

- **macOS 26.0+** on Apple Silicon (M1 or later)
- **iOS 26.0+** on A14 Bionic or later
- **visionOS 3.0+**

Key capabilities used:

| Feature | Why We Need It |
|---------|---------------|
| Unified Memory | Zero-copy CPU‚ÜîGPU data sharing |
| ArgumentTable | Efficient buffer binding |
| MTLSharedEvent | CPU/GPU synchronization |
| SIMD Group Operations | Warp-level reductions |
| Compute Pipelines | Shader execution |

---

## Known Limitations

VectorAccelerate is under active development. Current limitations:

| Limitation | Details |
|------------|---------|
| **GPU Distance Metrics** | Only `.euclidean` is fully GPU-accelerated. Other metrics fall back to CPU. |
| **Fused Top-K** | `FusedL2TopKKernel` works best with K ‚â§ 8 (private heap size). Larger K uses fallback strategies. |
| **IVF Index** | IVF support is functional but work-in-progress. Auto-routing to flat search helps with small datasets. |
| **Quantization** | Scalar quantization (SQ8, SQ4) is available for IVF indexes only. |

---

## Let's Begin

Ready to unleash the GPU?

**[‚Üí Chapter 1: GPU Fundamentals](./01-GPU-Fundamentals/README.md)**

---

*VectorAccelerate Learning Guide ‚Ä¢ Volume 3 of the VSK Educational Series ‚Ä¢ Dec 2024*
