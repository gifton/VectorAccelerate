# VectorAccelerate

**GPU-Accelerated Vector Operations for VectorCore â€” Metal 4 Edition**

VectorAccelerate provides high-performance GPU acceleration for vector operations, serving as the computational backbone for the VectorCore ecosystem. By leveraging Metal 4's compute shaders, unified command encoding, and Apple Silicon's unified memory architecture, VectorAccelerate delivers up to 100x speedups for large-scale vector operations.

> **âš ï¸ Version 0.3.0**: Requires **Metal 4** (macOS 26.0+, iOS 26.0+, visionOS 3.0+). For older OS support, use VectorAccelerate 0.2.x

## ğŸ¯ Purpose

VectorAccelerate exists to solve a critical performance bottleneck in vector-based machine learning applications on in storage, thermal, and power constrained environments. While VectorCore provides an elegant Swift interface for vector operations, VectorAccelerate ensures these operations run at maximum speed by:

- **Metal 4 Acceleration**: Leveraging unified command encoding and tensor operations
- **Optimized Kernels**: Hand-tuned Metal shaders for specific dimensions (512, 768, 1536)
- **Memory Efficiency**: Advanced quantization and compression techniques
- **ML Integration**: Experimental learned distance metrics with MLTensor support
- **Seamless Integration**: Drop-in acceleration for VectorCore operations

## ğŸ›ï¸ Two-Layer API Architecture

VectorAccelerate provides **two complementary API layers** designed for different use cases:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: High-Level API (AcceleratedVectorIndex)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Complete vector search solution                     â”‚  â”‚
â”‚  â”‚  â€¢ insert(), search(), remove(), compact()             â”‚  â”‚
â”‚  â”‚  â€¢ Automatic GPU/CPU routing                           â”‚  â”‚
â”‚  â”‚  â€¢ Best for: Applications needing similarity search    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Low-Level API (25+ GPU Kernel Primitives)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Direct GPU kernel access                            â”‚  â”‚
â”‚  â”‚  â€¢ Distance, Selection, Quantization, Matrix kernels   â”‚  â”‚
â”‚  â”‚  â€¢ Pipeline composition with encode() methods          â”‚  â”‚
â”‚  â”‚  â€¢ Best for: Custom ML pipelines, fine-grained control â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer 1: High-Level API

For most applications, use `AcceleratedVectorIndex`:

```swift
import VectorAccelerate

// Create a flat index (exact search)
let index = try await AcceleratedVectorIndex(
    configuration: .flat(dimension: 768, capacity: 10_000)
)

// Insert vectors and get handles
let handle = try await index.insert(embedding)

// Search for nearest neighbors
let results = try await index.search(query: queryVector, k: 10)
for result in results {
    print("Handle: \(result.handle), Distance: \(result.distance)")
}
```

### Layer 2: Low-Level Kernel Primitives

For custom pipelines or maximum control, use kernels directly:

```swift
import VectorAccelerate

let context = try await Metal4Context()

// Use individual kernels
let l2Kernel = try await L2DistanceKernel(context: context)
let distances = try await l2Kernel.compute(
    queries: queryVectors,
    database: databaseVectors,
    computeSqrt: true
)

// Compose kernels in a pipeline
let normKernel = try await L2NormalizationKernel(context: context)
let cosineKernel = try await CosineSimilarityKernel(context: context)
let topKKernel = try await TopKSelectionKernel(context: context)

try await context.executeAndWait { _, encoder in
    normKernel.encode(into: encoder, ...)
    encoder.memoryBarrier(scope: .buffers)
    cosineKernel.encode(into: encoder, ...)
    encoder.memoryBarrier(scope: .buffers)
    topKKernel.encode(into: encoder, ...)
}
```

### Convenient Type Aliases

```swift
// Context
let context: GPUContext = try await Metal4Context()

// Distance kernels
let l2 = try await L2Kernel(context: context)
let cosine = try await CosineKernel(context: context)
let dot = try await DotKernel(context: context)

// Selection kernels
let topK = try await TopKKernel(context: context)
let fused = try await FusedTopKKernel(context: context)

// Quantization kernels
let binary = try await BinaryQuantKernel(context: context)
let scalar = try await ScalarQuantKernel(context: context)
```

## ğŸ“¦ Requirements

### System Requirements
- **macOS 26.0+** / **iOS 26.0+** / **tvOS 26.0+** / **visionOS 3.0+**
- **Metal 4** capable device (Apple Silicon)
- **Swift 6.0+**

### Dependencies
- **VectorCore 0.1.6+**: The foundational vector mathematics package

### Products
- **VectorAccelerate**: Core GPU acceleration library

## ğŸš€ Accelerated Operations

All kernels require a `Metal4Context` for initialization.

### Core Distance Metrics
- **L2 Distance** (`L2DistanceKernel`)
  - Euclidean distance with optional sqrt
  - Specialized kernels for dimensions 512, 768, 1536
  - Batch processing for multiple query-database pairs

- **Cosine Similarity** (`CosineSimilarityKernel`)
  - Pre-normalized and with-normalization variants
  - Output as similarity or distance (1 - similarity)
  - Optimized for high-dimensional embeddings

- **Dot Product** (`DotProductKernel`)
  - SIMD-optimized implementation
  - Automatic GEMV/GEMM path selection
  - Batch matrix-vector products

### Advanced Distance Metrics
- **Manhattan Distance** (`ManhattanDistanceKernel`) - L1 distance
- **Chebyshev Distance** (`ChebyshevDistanceKernel`) - Lâˆ distance
- **Hamming Distance** (`HammingDistanceKernel`) - Binary vector distances
- **Minkowski Distance** (`MinkowskiDistanceKernel`) - Generalized Lp distances

### Experimental ML Features
- **Learned Distance** (`LearnedDistanceKernel`)
  - Projection-based learned metrics
  - MLTensor integration for neural distance computation
  - Automatic fallback to standard L2 when unavailable

- **Attention Similarity** (`AttentionSimilarityKernel`)
  - Attention-weighted similarity computation
  - Multi-head attention support

- **Neural Quantization** (`NeuralQuantizationKernel`)
  - Learned quantization with neural networks
  - Adaptive codebook generation

### Vector Operations
- **L2 Normalization** (`L2NormalizationKernel`)
  - In-place and out-of-place normalization
  - Numerical stability for zero vectors
  - Batch normalization support

- **Element-wise Operations** (`ElementwiseKernel`)
  - Addition, subtraction, multiplication, division
  - Trigonometric functions (sin, cos, tan)
  - Power and exponential operations
  - Broadcasting support

### Selection & Sorting
- **Top-K Selection** (`TopKSelectionKernel`)
  - General purpose top-k with configurable k
  - Warp-level optimization for common k values
  - Streaming support for massive datasets

- **Fused L2 + Top-K** (`FusedL2TopKKernel`)
  - Combined distance computation and selection
  - Reduced memory bandwidth
  - Optimal for nearest neighbor search

- **Parallel Reduction** (`ParallelReductionKernel`)
  - Sum, min, max reduction
  - Mean and variance computation
  - Custom reduction operations

### Matrix Operations
- **Matrix Multiply (GEMM)** (`MatrixMultiplyKernel`)
  - Tiled implementation with shared memory (32Ã—32Ã—8 tiles)
  - Support for transposed inputs
  - Alpha/beta scaling (C = Î±Â·AÂ·B + Î²Â·C)

- **Matrix-Vector (GEMV)** (`MatrixVectorKernel`)
  - SIMD group optimizations
  - Row/column major support
  - Batch vector operations

- **Matrix Transpose** (`MatrixTransposeKernel`)
  - Tiled transpose for coalesced access
  - Optimized shared memory usage

- **Batch Matrix Operations** (`BatchMatrixKernel`)
  - Fused multiply-add with bias
  - Strided tensor operations

### Statistical Operations
- **Statistics** (`StatisticsKernel`)
  - Mean, variance, standard deviation
  - Skewness and kurtosis
  - Percentiles and quartiles
  - Running statistics updates

- **Histogram** (`HistogramKernel`)
  - Uniform, adaptive, and logarithmic binning
  - Multi-dimensional histograms
  - Kernel density estimation

### Quantization & Compression
- **Scalar Quantization** (`ScalarQuantizationKernel`)
  - INT8/INT4 quantization with scale and offset
  - Symmetric and asymmetric modes
  - Per-channel quantization

- **Binary Quantization** (`BinaryQuantizationKernel`)
  - 1-bit vector compression
  - Hamming distance on packed bits
  - 32x memory reduction

- **Product Quantization** (`ProductQuantizationKernel`)
  - Subspace decomposition
  - Codebook-based compression
  - Asymmetric distance computation

## ğŸ—‚ï¸ VectorIndexAcceleration

GPU-first vector index for high-performance similarity search.

### Quick Start

```swift
import VectorIndexAcceleration

// Create a GPU-accelerated flat index
let config = IndexConfiguration.flat(dimension: 768, capacity: 10_000)
let index = try await AcceleratedVectorIndex(configuration: config)

// Insert vectors
let handle = try await index.insert(embedding, metadata: ["type": "document"])

// Search (returns L2Â² distances - native GPU format)
let results = try await index.search(query: queryVector, k: 10)

// Filtered search
let filtered = try await index.search(query: queryVector, k: 10) { handle, meta in
    meta?["type"] == "document"
}
```

### Index Types

| Type | Use Case | Performance |
|------|----------|-------------|
| **Flat** | Small-medium datasets (<100K) | Exact results, ~0.3ms search |
| **IVF** | Large datasets (100K+) | Approximate, faster at scale |

### Handle Stability

`VectorHandle` instances are **stable** â€” they remain valid across `compact()` operations:

```swift
let handle = try await index.insert(embedding)

// Delete some vectors
try await index.remove(otherHandle)

// Compact to reclaim space
try await index.compact()

// Original handle still works!
let vector = try await index.vector(for: handle)  // âœ“ Valid
```

Internally, handles use an indirection table that maps stable IDs to storage slots, so compaction can relocate vectors without invalidating user-held handles.

### Performance

- **Insert**: ~21K vectors/sec (128D), ~3.7K vectors/sec (768D)
- **Search**: 0.30ms (128D), 0.73ms (768D) on 5K vectors
- **Sub-millisecond** latency for typical workloads

## ğŸ”§ Installation

### Swift Package Manager

Add VectorAccelerate to your `Package.swift`:

```swift
dependencies: [
    .package(url: "https://github.com/gifton/VectorAccelerate.git", from: "0.3.0"),
    .package(url: "https://github.com/gifton/VectorCore.git", from: "0.1.6")
],
targets: [
    .target(
        name: "YourTarget",
        dependencies: [
            "VectorAccelerate",
            "VectorCore",
            // Add if using GPU-accelerated indices:
            "VectorIndexAcceleration"
        ]
    )
]
```

> **Note**: Version 0.3.0+ requires Metal 4. For macOS 15 / iOS 18 support, use version 0.1.x.

## ğŸ“ Getting Started

### Basic Usage

```swift
import VectorAccelerate
import VectorCore

// Initialize Metal 4 context (async)
let context = try await Metal4Context()
let l2Kernel = try await L2DistanceKernel(context: context)

// Prepare your vectors
let queries = [[Float]](repeating: [Float](repeating: 0.5, count: 768), count: 10)
let database = [[Float]](repeating: [Float](repeating: 0.3, count: 768), count: 1000)

// Compute distances on GPU
let distances = try await l2Kernel.compute(
    queries: queries,
    database: database,
    computeSqrt: true  // For Euclidean distance
)

print("Computed \(distances.count) x \(distances[0].count) distances on GPU")
```

### Using with VectorCore Types

```swift
import VectorCore

// Create VectorCore vectors
let vector1 = Vector<D768>([Float](repeating: 1.0, count: 768))
let vector2 = Vector<D768>([Float](repeating: 0.5, count: 768))

// Use GPU-accelerated operations
let context = try await Metal4Context()
let cosineSim = try await CosineSimilarityKernel(context: context)
let similarity = try await cosineSim.compute(
    queries: [vector1],
    database: [vector2]
)
```

### Advanced: Fused Operations

```swift
// Fused L2 distance + Top-K selection (single kernel execution)
let context = try await Metal4Context()
let fusedKernel = try await FusedL2TopKKernel(context: context)

let result = try await fusedKernel.compute(
    queries: queries,
    database: database,
    k: 10
)

// Result contains top-10 nearest neighbors for each query
print("Found \(result.indices[0].count) nearest neighbors per query")
```

### Learned Distance (Experimental ML)

```swift
// Use learned projections for distance computation
let context = try await Metal4Context()
let config = AccelerationConfiguration(enableExperimentalML: true)
let service = try await LearnedDistanceService(context: context, configuration: config)

// Load projection weights (e.g., from a trained model)
try await service.loadProjection(
    from: weightsURL,
    inputDim: 768,
    outputDim: 128
)

// Compute distances with learned projection
let (distances, mode) = try await service.computeL2(
    queries: queries,
    database: database
)

print("Computed using \(mode) mode")  // .learned or .standard (fallback)
```

### Matrix Operations

```swift
// GPU-accelerated matrix multiplication
let context = try await Metal4Context()
let matrixKernel = try await MatrixMultiplyKernel(context: context)

let a = Matrix.random(rows: 1024, columns: 512)
let b = Matrix.random(rows: 512, columns: 256)

let result = try await matrixKernel.multiply(a: a, b: b)
print("Result: \(result.rows) x \(result.columns)")
```

### VectorCore Integration

VectorAccelerate provides GPU-accelerated `DistanceProvider` implementations:

```swift
import VectorAccelerate
import VectorCore

// Create a kernel-backed distance provider
let context = try await Metal4Context()
let provider = try await L2KernelDistanceProvider(context: context)

// Works with VectorCore's DynamicVector
let v1 = DynamicVector([1.0, 0.0, 0.0])
let v2 = DynamicVector([0.0, 1.0, 0.0])

let distance = try await provider.distance(from: v1, to: v2, metric: .euclidean)

// Batch distance computation
let candidates = [v1, v2, DynamicVector([0.5, 0.5, 0.0])]
let distances = try await provider.batchDistance(from: v1, to: candidates, metric: .euclidean)
```

**Universal Distance Provider** - handles all metrics automatically:

```swift
// Dispatches to the optimal kernel for each metric
let provider = await context.universalDistanceProvider()

let euclidean = try await provider.distance(from: v1, to: v2, metric: .euclidean)
let cosine = try await provider.distance(from: v1, to: v2, metric: .cosine)
let manhattan = try await provider.distance(from: v1, to: v2, metric: .manhattan)
```

**Available Distance Providers:**

| Provider | Metric | Features |
|----------|--------|----------|
| `L2KernelDistanceProvider` | Euclidean | Dimension-optimized (384, 512, 768, 1536) |
| `CosineKernelDistanceProvider` | Cosine | Auto-normalization |
| `DotProductKernelDistanceProvider` | Dot Product | GEMV optimization |
| `MinkowskiKernelDistanceProvider` | Manhattan, Chebyshev | Configurable p-norm |
| `JaccardKernelDistanceProvider` | Jaccard | Set similarity |
| `HammingKernelDistanceProvider` | Hamming | Binary vectors |
| `UniversalKernelDistanceProvider` | All | Auto-dispatch |

## ğŸ“‹ Choosing the Right Kernel

### Distance Computation

| Use Case | Recommended Kernel | Notes |
|----------|-------------------|-------|
| Nearest neighbor search | `L2DistanceKernel` | Best for embeddings |
| Semantic similarity | `CosineSimilarityKernel` | Direction-based |
| Maximum inner product | `DotProductKernel` | For unnormalized vectors |
| Sparse data / sets | `JaccardDistanceKernel` | Document fingerprints |
| Binary vectors | `HammingDistanceKernel` | After binary quantization |
| Custom Lp norm | `MinkowskiDistanceKernel` | Configurable p |

### Selection

| Use Case | Recommended Kernel | Notes |
|----------|-------------------|-------|
| Standard top-k | `TopKSelectionKernel` | General purpose |
| Memory-constrained | `FusedL2TopKKernel` | Avoids full distance matrix, auto-fallback for K > 8 |
| Large datasets | `FusedL2TopKKernel` | Uses chunked GPU merge for memory efficiency |
| SIMD-optimized | `WarpOptimizedSelectionKernel` | Small k values (k â‰¤ 32) |

> **Note**: `StreamingTopKKernel` is deprecated due to known correctness issues. Use `FusedL2TopKKernel` which automatically handles large datasets via chunked GPU merge.

### Quantization

| Use Case | Recommended Kernel | Compression |
|----------|-------------------|-------------|
| Fast approximate search | `BinaryQuantizationKernel` | 32x |
| Quality-preserving | `ScalarQuantizationKernel` | 4-8x |
| High compression | `ProductQuantizationKernel` | 32-64x |

## ğŸ—ï¸ Architecture

### Kernel Organization

VectorAccelerate is organized into specialized Metal 4 kernels:

```
VectorAccelerate/
â”œâ”€â”€ Core/                    # Metal 4 infrastructure
â”‚   â”œâ”€â”€ Metal4Context.swift      # Unified context management
â”‚   â”œâ”€â”€ Metal4ComputeEngine.swift # Command encoding
â”‚   â”œâ”€â”€ ResidencyManager.swift    # Memory residency
â”‚   â”œâ”€â”€ PipelineCache.swift       # Pipeline state caching
â”‚   â””â”€â”€ TensorManager.swift       # Tensor operations
â”œâ”€â”€ Kernels/
â”‚   â””â”€â”€ Metal4/              # All Metal 4 kernel implementations
â”‚       â”œâ”€â”€ L2DistanceKernel.swift
â”‚       â”œâ”€â”€ CosineSimilarityKernel.swift
â”‚       â”œâ”€â”€ MatrixMultiplyKernel.swift
â”‚       â””â”€â”€ ... (20+ kernels)
â”œâ”€â”€ Metal/
â”‚   â””â”€â”€ Shaders/             # Metal shader source files (.metal)
â””â”€â”€ Operations/              # High-level operation orchestration
```

### Metal 4 Features Used

- **Unified Command Encoding**: Single encoder for compute + blit operations
- **Residency Sets**: Explicit memory management for optimal GPU utilization
- **Argument Tables**: Efficient parameter passing to shaders
- **Pipeline Harvesting**: Background pipeline compilation

### Performance Optimizations

1. **Dimension-Specific Kernels**: Hand-tuned for 512, 768, 1536 dimensions
2. **Tiled Algorithms**: 32Ã—32Ã—8 tiles optimized for Apple Silicon cache
3. **SIMD Operations**: Leveraging float4 and simdgroup matrix operations
4. **Fused Kernels**: Combined distance + selection for reduced memory bandwidth
5. **Async Pipeline Creation**: Non-blocking kernel initialization

## ğŸ“Š Performance

Typical speedups over CPU implementations:

| Operation | Vector Size | CPU Time | GPU Time | Speedup |
|-----------|------------|----------|----------|---------|
| L2 Distance | 1M Ã— 10K | 12.3s | 0.15s | 82x |
| Cosine Similarity | 100K Ã— 100K | 8.7s | 0.09s | 97x |
| Matrix Multiply | 2048 Ã— 2048 | 1.8s | 0.03s | 60x |
| Top-K Selection | 1M, k=100 | 0.9s | 0.02s | 45x |
| INT8 Quantization | 10M vectors | 2.1s | 0.04s | 52x |

## âš ï¸ Known Limitations

### Fused L2 Top-K Kernel

The `FusedL2TopKKernel` uses different strategies based on K:

| K Range | Strategy | Memory |
|---------|----------|--------|
| K â‰¤ 8 | Fused single-pass | O(Q Ã— K) - no distance matrix |
| 8 < K â‰¤ 32 | Two-pass with warp selection | O(Q Ã— N) distance matrix |
| K > 32 | Two-pass with standard selection | O(Q Ã— N) distance matrix |
| Large N | Chunked GPU merge | Bounded by `maxDistanceMatrixBytes` |

For K > 8, the kernel automatically falls back to a two-pass approach. For very large datasets, it uses chunked processing with GPU-side merge to stay within memory bounds.

### IVF Index (Work in Progress)

The IVF index has known correctness issues being addressed:
- Batch search may not correctly isolate per-query candidate lists
- Training with many deletions may cause duplicate entries

For production use, prefer the **flat index** until these are resolved (see `QUALITY_IMPROVEMENT_ROADMAP.md` P0.1-P0.4).

### Metal Shader Compilation

Shaders are validated with `-std=metal3.0` in CI for syntax checking, but the runtime requires **Metal 4** features (macOS 26.0+). This is intentional â€” Metal 3 syntax is a subset of Metal 4.

## ğŸ§ª Testing

Run the test suite:

```bash
swift test
```

Run performance benchmarks:

```bash
swift run VectorAccelerateBenchmarks
```

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Areas for Contribution
- Additional distance metrics
- Optimizations for new Apple Silicon features (M4, M5)
- Enhanced ML integration
- Performance benchmarks and comparisons

## ğŸ“„ License

VectorAccelerate is available under the MIT license. See [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Apple's Metal team for the excellent GPU framework
- The VectorCore team for the foundational vector library
- Contributors to the scientific computing community

## ğŸ“š Related Projects

- [VectorCore](https://github.com/gifton/VectorCore) - Core vector mathematics
- [VectorIndex](https://github.com/gifton/VectorIndex) - CPU-based vector indexing algorithms
- [VectorDatabase](https://github.com/gifton/VectorDatabase) - Complete vector database solution

---

**Requirements**: VectorAccelerate 0.3.0+ requires **Metal 4** (macOS 26.0+, iOS 26.0+, visionOS 3.0+) and Apple Silicon. For older OS versions, use VectorAccelerate 0.1.x.
