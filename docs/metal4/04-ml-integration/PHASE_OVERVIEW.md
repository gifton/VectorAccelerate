# Phase 4: ML/Tensor Integration (Experimental)

> **IMPORTANT:** This phase is **optional** and **experimental**. Core Metal 4 migration is complete after Phase 3. Phase 4 features may ship in a later minor release or be deferred based on performance analysis and user demand.

## Objective

Explore and optionally implement native `MTLTensor` support and inline ML operations in Metal shaders. This enables advanced features like learned distance metrics, neural quantization, and attention-based similarity.

## Dependencies

- Phase 3 complete (MSL 4.0 shaders, Metal4Compiler)
- Feature-gated behind `Metal4Capabilities.supportsMLTensor`

## Scope Clarification

### In Scope (Phase 4)

- **Inline tensor operations** in existing shader patterns
- **Small model inference** directly in compute shaders (projections, activations)
- **TensorBuffer management** for weight storage
- **Performance analysis** vs existing approaches

### Out of Scope (Future Work)

- Large model inference (use CoreML/MPSGraph)
- Training or gradient computation
- Complex transformer architectures
- Integration with CoreML pipeline

### Decision Points

Before implementing each sub-task, evaluate:

1. **Performance benefit** - Does inline tensor beat existing CPU/MPS approach?
2. **Complexity cost** - How much code complexity does this add?
3. **User demand** - Do downstream consumers need this feature?
4. **Hardware availability** - Do target devices support MLTensor?

---

## Tasks

| Task | Description | Priority | Status |
|------|-------------|----------|--------|
| task-tensor-manager.md | MTLTensor buffer creation and management | P1 | **Complete** |
| task-learned-distance.md | Learned projection for distance metrics | P2 | **Complete** |
| task-neural-quantization.md | Encoder/decoder quantization | P3 | **Complete** |
| task-attention-similarity.md | Inline attention for semantic similarity | P3 | **Complete** |

---

## Feature Gating

All ML features must be gated:

```swift
// Check before using ML features
guard context.capabilities.supportsMLTensor else {
    // Fall back to standard implementation
    return try await standardDistance(queries, database)
}

// Or throw for required features
guard context.capabilities.supportsMLTensor else {
    throw Metal4Error.mlTensorNotAvailable
}
```

### Configuration Flag

```swift
public struct AccelerationConfiguration {
    // ... existing fields ...

    /// Enable experimental ML features (Phase 4)
    /// Default: false
    /// Requires: Metal4Capabilities.supportsMLTensor == true
    public var enableExperimentalML: Bool = false
}
```

---

## ML Command Encoder Consideration

Metal 4 also includes an ML-specific command encoder for larger models. This is **not** the same as inline tensor operations.

### ML Command Encoder (Not Phase 4 Scope)

```swift
// For large models - use CoreML/MPSGraph integration instead
let mlEncoder = commandBuffer.makeMLComputeCommandEncoder()
mlEncoder.encode(mlGraph, ...)
```

### Inline Tensor (Phase 4 Scope)

```metal
// Small operations directly in compute shaders
#include <metal_tensor>

kernel void projectedDistance(...) {
    auto inputTensor = MTLTensor<float, dextents<1>>(input + tid * dim, dim);
    auto projected = matmul(inputTensor, *projectionWeights);
    // ...
}
```

**Recommendation:** Start with inline tensor for small projections. Defer ML encoder integration to future work if large model integration is needed.

---

## Kernel Patterns

### Pattern 1: Learned Distance Metric

**Use case:** Project vectors through learned transformation before computing distance.

```metal
kernel void learnedL2Distance(
    device const float* queries [[buffer(0)]],
    device const float* database [[buffer(1)]],
    device const MTLTensor<float, 2>* projection [[buffer(2)]],
    device float* distances [[buffer(3)]],
    constant DistanceParams& params [[buffer(4)]],
    uint2 gid [[thread_position_in_grid]]
) {
    // Load vectors
    auto query = loadVector(queries, gid.y, params.dimension);
    auto dbVec = loadVector(database, gid.x, params.dimension);

    // Project through learned weights
    auto projQuery = matmul(query, *projection);
    auto projDb = matmul(dbVec, *projection);

    // Compute L2 in projected space
    float dist = computeL2(projQuery, projDb, params.projectedDim);

    distances[gid.y * params.numDatabase + gid.x] = dist;
}
```

**Fallback:** Standard L2 distance without projection.

### Pattern 2: Neural Quantization

**Use case:** Learned encoder/decoder for better quantization fidelity.

```metal
kernel void neuralEncode(
    device const float* vectors [[buffer(0)]],
    device const MTLTensor<float, 2>* encoder [[buffer(1)]],
    device uint8_t* codes [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    auto vec = loadVector(vectors, gid, dim);

    // Encode to latent space
    auto latent = relu(matmul(vec, *encoder));

    // Quantize
    for (int i = 0; i < latentDim; i++) {
        codes[gid * latentDim + i] = uint8_t(clamp(latent[i] * 255.0, 0.0, 255.0));
    }
}
```

**Fallback:** Standard scalar quantization.

### Pattern 3: Attention-Based Similarity

**Use case:** Cross-attention between query and database items.

```metal
kernel void attentionSimilarity(
    device const float* queries [[buffer(0)]],
    device const float* keys [[buffer(1)]],
    device const MTLTensor<float, 2>* Wq [[buffer(2)]],
    device const MTLTensor<float, 2>* Wk [[buffer(3)]],
    device float* similarities [[buffer(4)]],
    constant AttentionParams& params [[buffer(5)]],
    uint2 gid [[thread_position_in_grid]]
) {
    // Project query and key
    auto q = matmul(loadVector(queries, gid.y), *Wq);
    auto k = matmul(loadVector(keys, gid.x), *Wk);

    // Scaled dot-product
    float score = dot(q, k) / sqrt(float(params.headDim));

    similarities[gid.y * params.numKeys + gid.x] = score;
}
```

**Fallback:** Standard cosine similarity.

---

## TensorManager Design

```swift
/// Manages MTLTensor buffers for ML weights
public actor TensorManager {
    private let device: MTLDevice
    private var tensors: [String: TensorBuffer] = [:]

    public struct TensorBuffer {
        let buffer: any MTLBuffer
        let shape: [Int]
        let dataType: MTLDataType
    }

    /// Load weights from file
    public func loadWeights(
        from url: URL,
        name: String,
        shape: [Int]
    ) async throws -> TensorBuffer {
        let data = try Data(contentsOf: url)
        let size = shape.reduce(1, *) * MemoryLayout<Float>.size

        guard data.count == size else {
            throw Metal4Error.tensorSizeMismatch(expected: size, got: data.count)
        }

        guard let buffer = device.makeBuffer(bytes: Array(data),
                                              length: size,
                                              options: .storageModeShared) else {
            throw VectorError.bufferAllocationFailed(size: size)
        }

        let tensor = TensorBuffer(buffer: buffer, shape: shape, dataType: .float)
        tensors[name] = tensor
        return tensor
    }

    /// Get loaded tensor
    public func getTensor(name: String) -> TensorBuffer? {
        tensors[name]
    }
}
```

---

## Performance Evaluation

Before shipping any Phase 4 feature, measure:

### Metrics to Compare

| Metric | Baseline | With ML | Target |
|--------|----------|---------|--------|
| Throughput (queries/sec) | Existing | New | >= 90% of baseline |
| Latency (ms/query) | Existing | New | <= 110% of baseline |
| Memory usage (MB) | Existing | New + weights | Document overhead |
| Accuracy | N/A | vs CPU reference | Within 1e-4 |

### Benchmark Suite

```swift
class MLIntegrationBenchmarks: XCTestCase {
    func testLearnedDistanceVsStandard() async throws {
        // Compare throughput
        let standardThroughput = try await benchmarkStandardL2()
        let learnedThroughput = try await benchmarkLearnedL2()

        // Learned should be at least 90% as fast
        XCTAssertGreaterThan(learnedThroughput, standardThroughput * 0.9)
    }

    func testNeuralQuantizationQuality() async throws {
        // Compare reconstruction error
        let scalarError = try await measureScalarQuantizationError()
        let neuralError = try await measureNeuralQuantizationError()

        // Neural should have lower error
        XCTAssertLessThan(neuralError, scalarError)
    }
}
```

---

## Rollout Strategy

### Phase 4a: Foundation (Optional)

- Implement TensorManager
- Add MLTensor feature detection
- Create benchmark harness
- **Ship:** Configuration flag, no user-visible features

### Phase 4b: Learned Distance (Optional)

- Implement learned projection kernel
- Provide sample weights format
- Document API
- **Ship:** If benchmarks positive, behind feature flag

### Phase 4c: Advanced Features (Future)

- Neural quantization
- Attention similarity
- **Ship:** Based on user demand

---

## Completion Criteria

- [x] TensorManager implemented
- [x] Feature gating works correctly
- [x] At least one ML kernel implemented (learned distance)
- [x] Neural Quantization kernel implemented (encoder/decoder + INT8)
- [x] Attention Similarity kernel implemented (single/multi-head)
- [x] Benchmark suite shows acceptable performance
- [x] Fallbacks work when MLTensor unavailable
- [x] Documentation explains trade-offs

---

## Benchmark Results (2025-11-30)

### Neural Quantization Performance

| Config | Compression | Throughput | Memory Savings |
|--------|-------------|------------|----------------|
| 768 ‚Üí 128 (balanced) | 24x | 159K vectors/sec | 95.8% |
| 768 ‚Üí 64 (high compression) | 48x | ~150K vectors/sec | 97.9% |
| 384 ‚Üí 64 (MiniLM) | 24x | ~200K vectors/sec | 95.8% |

**Scaling Behavior:**
```
Count   | Time (ms)  | Throughput (vec/s)
--------|------------|-------------------
    100 |       5.57 |             17,960
    500 |      12.74 |             39,253
   1000 |      13.72 |             72,899
   5000 |      32.99 |            151,556
  10000 |      62.88 |            159,029
```

### Attention Similarity Performance

| Config | Pairs Tested | Throughput | Per-Query Latency |
|--------|--------------|------------|-------------------|
| 768 ‚Üí 64 (single-head) | 100K | 956K pairs/sec | 1.08 ms |
| 384 ‚Üí 64 (MiniLM) | 100K | ~1.2M pairs/sec | 0.83 ms |
| 768 ‚Üí 64 (12-head) | 100K | ~300K pairs/sec | 3.3 ms |

**Scaling Behavior:**
```
Keys    | Time (ms)  | Throughput (pairs/s) | Per-query (ms)
--------|------------|----------------------|--------------
    100 |      22.73 |              439,857 |         0.227
    500 |      54.48 |              917,835 |         0.545
   1000 |     108.45 |              922,078 |         1.085
   5000 |     512.38 |              975,833 |         5.124
  10000 |    2488.40 |              401,864 |        24.884
```

### Trade-offs & Recommendations

**Neural Quantization:**
- ‚úÖ Best for: Large vector stores (>10K vectors) needing memory reduction
- ‚úÖ 24x compression with INT8 latent codes
- ‚ö†Ô∏è Trade-off: Encoding cost (~0.4ms per 1000 vectors)
- üí° Ideal when: Vectors encoded once, queried many times

**Attention Similarity:**
- ‚úÖ Best for: Asymmetric query-document retrieval
- ‚úÖ ~1M pairs/sec throughput (single-head)
- ‚ö†Ô∏è Trade-off: Projection cost for learned Wq/Wk weights
- üí° Ideal when: Domain-specific similarity patterns needed

**Standard L2 vs Learned Distance:**
- Standard L2 is faster for small databases (<10K vectors)
- Learned distance benefits from pre-computed projections
- Break-even point depends on projection reuse factor

## Risk Mitigation

- All features behind flags - can disable if issues found
- Fallbacks for all ML-enhanced operations
- Clear performance documentation
- Don't block core migration on Phase 4

## Files Modified

- `Core/TensorManager.swift` ‚Üí **COMPLETE** - Manages MTLTensor buffers, weight loading, shape management
- `Core/Metal4Capabilities.swift` ‚Üí Already has MLTensor detection
- `Metal/Shaders/LearnedDistance.metal` ‚Üí **COMPLETE** - Learned projection kernels for L2 and cosine
- `Kernels/LearnedDistanceKernel.swift` ‚Üí **COMPLETE** - Swift wrapper with TensorManager integration
- `Core/LearnedDistanceService.swift` ‚Üí **COMPLETE** - Unified service with automatic fallback
- `Configuration/AccelerationConfiguration.swift` ‚Üí **UPDATED** - Added `enableExperimentalML` flag
- `Core/Types.swift` ‚Üí **UPDATED** - Extended TensorShape with factory methods
- `Kernels/Metal4/Metal4NeuralQuantizationKernel.swift` ‚Üí **COMPLETE** - Learned encoder/decoder quantization
- `Metal/Shaders/NeuralQuantization.metal` ‚Üí **COMPLETE** - Neural encode/decode kernels with INT8 quantization
- `Kernels/Metal4/Metal4AttentionSimilarityKernel.swift` ‚Üí **COMPLETE** - Attention-based similarity with learned Wq/Wk
- `Metal/Shaders/AttentionSimilarity.metal` ‚Üí **COMPLETE** - Single/multi-head attention similarity kernels

---

## Decision Log

| Date | Decision | Rationale |
|------|----------|-----------|
| 2025-11-29 | Implement Phase 4a foundation | Phase 3 complete, proceeding with ML integration |
| 2025-11-29 | Use buffer-based weights vs MTLTensor | MTLTensor inline ops require Metal 4 SDK, buffer approach works on all Metal versions |
| 2025-11-29 | Create LearnedDistanceService | Provides unified API with automatic fallback to standard L2 |
| 2025-11-30 | Implement Neural Quantization | Provides better compression quality than scalar/PQ for learned representations |
| 2025-11-30 | Implement Attention Similarity | Cross-attention for asymmetric/domain-specific similarity patterns |
| 2025-11-30 | Performance benchmarks complete | Neural Quantization: 159K vec/s, Attention: 956K pairs/s |
| 2025-11-30 | Phase 4 complete | All completion criteria met |
